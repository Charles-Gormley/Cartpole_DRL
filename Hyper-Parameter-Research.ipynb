{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from buffer import Buffer\n",
    "from ou_noise import OUActionNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(env, std_dev, actor_lr, critic_lr, gamma, tau, total_episodes):\n",
    "    ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "    actor_model = get_actor()\n",
    "    critic_model = get_critic()\n",
    "\n",
    "    target_actor = get_actor()\n",
    "    target_critic = get_critic()\n",
    "\n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "    critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "    actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "    buffer = Buffer(50000, 64)\n",
    "\n",
    "    ep_reward_list = []\n",
    "    avg_reward_list = []\n",
    "\n",
    "    for ep in range(total_episodes):\n",
    "        prev_state = env.reset()\n",
    "        episodic_reward = 0\n",
    "\n",
    "        print(\"Episode: \"+ str(ep))\n",
    "        while True:\n",
    "            with tf.device('GPU:0'):\n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state[0], dtype=tf.float32), 0)\n",
    "\n",
    "                action = policy(tf_prev_state, ou_noise)\n",
    "\n",
    "                state, reward, terminated, truncated, info = env.step(action)\n",
    "                state = tf.expand_dims(state, 0)\n",
    "\n",
    "                buffer.record((prev_state[0], action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                buffer.learn()\n",
    "                update_target(target_actor.variables, actor_model.variables, tau)\n",
    "                update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "        ep_reward_list.append(episodic_reward)\n",
    "\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        avg_reward_list.append(avg_reward)\n",
    "\n",
    "    return ep_reward_list, avg_reward_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "ep_reward_list, avg_reward_list = train_actor_critic(env, std_dev=0.2, actor_lr=0.001, critic_lr=0.002, gamma=0.99, tau=0.005, total_episodes=100)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
